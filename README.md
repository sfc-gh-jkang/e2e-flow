# E2E Flow - Prefect Worker on Prefect Cloud

**Based on:** [cortex-cost-app-spcs](https://github.com/sfc-gh-jkang/cortex-cost-app-spcs)

**Reference:** Prefect docs for ECS worker setup: <https://docs.prefect.io/integrations/prefect-aws/ecs-worker/manual-deployment>

## üéØ What This Does

**EVE Online Market Data ETL Pipeline** that:

- üìä Pulls daily market data from EVE Online ESI API
- ‚úÖ Validates data with Pandera schemas
- üíæ Loads to **Crunchy Bridge PostgreSQL** OR **Snowflake PostgreSQL**
- üîÑ Supports upsert (merge) operations with primary keys
- üê≥ Runs in Docker containers on Google Cloud VM

This project runs **Prefect workers on a Google VM** with two work pools:

- **`google-vm`** - Process-based worker for direct execution
- **`google-vm-docker`** - Docker-based worker for containerized flows

Features:

- ‚úÖ Connects to Prefect Cloud over HTTPS (port 443)
- ‚úÖ Executes flow runs from your Prefect workspace
- ‚úÖ Dual database support: Crunchy Bridge + Snowflake PostgreSQL
- ‚úÖ Uses UV for ultra-fast Python package management

## ‚úÖ Infrastructure Setup

This project supports multiple deployment methods:

1. **Google VM with Docker** (current active setup) - See `google_compute_quickstart.md`
2. **Snowflake SPCS** - Container services within Snowflake

Both use UV (ultra-fast Python package installer) and Docker containers.

### üìÅ Project Structure

```text
e2e-flow/
‚îú‚îÄ‚îÄ Dockerfile                 # Ubuntu-based container with UV
‚îú‚îÄ‚îÄ docker-compose.yml         # Local development setup
‚îú‚îÄ‚îÄ prefect.yaml              # Prefect deployments configuration
‚îú‚îÄ‚îÄ prefect_test.py           # EVE Online ETL flow and tasks
‚îú‚îÄ‚îÄ docker_cleanup.py         # Docker cleanup maintenance flow
‚îú‚îÄ‚îÄ main.py                   # Prefect worker entry point
‚îú‚îÄ‚îÄ pyproject.toml            # Python project configuration
‚îú‚îÄ‚îÄ google_compute_quickstart.md  # Google VM setup guide
‚îÇ
‚îú‚îÄ‚îÄ crunchy_bridge_connection/    # PostgreSQL utilities
‚îÇ   ‚îú‚îÄ‚îÄ connection.py         # Database connection (Crunchy + Snowflake)
‚îÇ   ‚îú‚îÄ‚îÄ csv_loader.py         # CSV loading, upsert, and CLI tools
‚îÇ   ‚îî‚îÄ‚îÄ README.md             # Module documentation
‚îÇ
‚îú‚îÄ‚îÄ eve_online_data/          # EVE Online market data module
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py           # Data fetching and validation
‚îÇ   ‚îî‚îÄ‚îÄ *.csv                 # Downloaded market data files
‚îÇ
‚îú‚îÄ‚îÄ docs/                     # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ QUICKSTART.md         # Quick start guide
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT_GUIDE.md   # Detailed deployment guide
‚îÇ   ‚îî‚îÄ‚îÄ ...                   # Additional guides
‚îÇ
‚îî‚îÄ‚îÄ scripts/                  # Deployment scripts
    ‚îú‚îÄ‚îÄ deploy.sh             # SPCS deployment script
    ‚îú‚îÄ‚îÄ build-and-push-github.sh  # GitHub Container Registry script
    ‚îî‚îÄ‚îÄ test-local-container.sh   # Local testing script
```

## üìä EVE Online ETL Pipeline

### Prefect Deployments

The project includes 4 Prefect deployments configured in `prefect.yaml`:

| Deployment | Target DB | Work Pool | Schedule |
|------------|-----------|-----------|----------|
| `eve-market-etl` | Crunchy | `google-vm` | 6 AM UTC |
| `docker-eve-market-etl` | Crunchy | `google-vm-docker` | 7 AM UTC |
| `docker-sf-eve-market-etl` | Snowflake | `google-vm-docker` | 8 AM UTC |
| `docker-cleanup` | N/A | `google-vm` | 8 AM UTC |

### Running the ETL Locally

```bash
# Run ETL to Crunchy Bridge (default)
uv run python -c "from prefect_test import load_eve_market_data; load_eve_market_data()"

# Run ETL to Snowflake PostgreSQL
uv run python -c "from prefect_test import load_eve_market_data; load_eve_market_data(crunchy_or_snowflake='snowflake')"
```

### Testing Database Connections

```bash
# Test Crunchy Bridge connection
uv run python -c "from crunchy_bridge_connection.connection import test_connection; test_connection()"

# Test Snowflake PostgreSQL connection
uv run python -c "from crunchy_bridge_connection.connection import test_connection; test_connection(crunchy_or_snowflake='snowflake')"
```

### CSV Loader CLI

```bash
# Load CSV to table (uses COPY - fast bulk load)
uv run python -m crunchy_bridge_connection.csv_loader load <csv_file> <table_name> --schema eve_online

# Upsert CSV to table (uses INSERT ON CONFLICT - merges data)
uv run python -m crunchy_bridge_connection.csv_loader upsert <csv_file> <table_name> \
  --primary-keys region_id,typeid,last_data --schema eve_online

# Pull table to CSV
uv run python -m crunchy_bridge_connection.csv_loader pull eve_online.eve_market_data

# Add --snowflake flag to target Snowflake instead of Crunchy
uv run python -m crunchy_bridge_connection.csv_loader pull eve_online.eve_market_data --snowflake
```

### Environment Variables

Create a `.env` file with:

```bash
# Crunchy Bridge PostgreSQL
PGHOST="p.EXAMPLE.db.postgresbridge.com"
PGDATABASE="postgres"
PGUSER="application"
PGPASSWORD="your-password"

# Snowflake PostgreSQL (wire protocol)
SNOWFLAKE_POSTGRES_HOST="your-account.snowflakecomputing.com"
SNOWFLAKE_POSTGRES_USER="your-user"
SNOWFLAKE_POSTGRES_PASSWORD="your-password"
SNOWFLAKE_POSTGRES_DATABASE="your-database"

# Prefect Cloud
PREFECT_API_URL="https://api.prefect.cloud/api/accounts/.../workspaces/..."
PREFECT_API_KEY="pnu_..."
```

---

## üöÄ Quick Start - Local Testing

**Option 1: Using the test script (recommended)**

```bash
# Build and run
./test-local-container.sh --build

# View logs
./test-local-container.sh --logs

# Open shell
./test-local-container.sh --shell

# Stop container
./test-local-container.sh --stop
```

**Option 2: Using Docker Compose**

```bash
docker-compose up --build
```

**Option 3: Using Docker directly**

```bash
docker build --platform linux/amd64 -t e2e-flow:latest .
docker run -p 8080:8080 e2e-flow:latest
```

### üêô GitHub Container Registry

Push your Docker image to GitHub Container Registry for easy sharing and deployment:

**Setup (one-time):**

```bash
# 1. Create a GitHub Personal Access Token
#    Go to: https://github.com/settings/tokens
#    Scopes: write:packages, read:packages

# 2. Export the token
export CR_PAT=your_token_here

# 3. Login to GitHub Container Registry
echo $CR_PAT | docker login ghcr.io -u your-github-username --password-stdin
```

**Build and push:**

```bash
# Build and push to GitHub Container Registry
./build-and-push-github.sh

# Build only (don't push)
./build-and-push-github.sh --local

# Build with custom tag
./build-and-push-github.sh --tag v1.0.0

# Build without cache
./build-and-push-github.sh --no-cache

# Show help
./build-and-push-github.sh --help
```

Your image will be available at: `ghcr.io/your-username/e2e-flow:latest`

### ‚òÅÔ∏è Snowflake SPCS Deployment

#### Prerequisites

- **Docker** installed and running
- **Snowflake CLI** installed ([Installation Guide](https://docs.snowflake.com/en/developer-guide/snowflake-cli/installation))
- **Snowflake account** with SPCS enabled
- **Appropriate privileges** (ACCOUNTADMIN or custom role with SPCS permissions)

#### Step 1: Install Snowflake CLI

```bash
# macOS (using Homebrew)
brew install snowflake-cli

# Or using pip
pip install snowflake-cli-labs

# Verify installation
snow --version
```

#### Step 2: Configure Snowflake Connection

```bash
# Add a new connection
snow connection add

# Or test existing connection
snow connection test -c default

# List available connections
snow connection list
```

#### Step 3: Set up Snowflake Infrastructure

Run the setup SQL in Snowflake (via Snowsight or SnowSQL):

```bash
# Option 1: Using Snowflake CLI
snow sql -f snowflake-setup.sql -c default

# Option 2: Copy and paste into Snowsight
# Open snowflake-setup.sql and execute the commands
```

This creates:

- Database: `E2E_FLOW_DB`
- Schemas: `IMAGE_SCHEMA`, `APP_SCHEMA`
- Image repository: `IMAGE_REPO`
- Compute pool: `E2E_FLOW_COMPUTE_POOL`
- Application stage: `APP_STAGE`

#### Step 4: Deploy to Snowflake

**First deployment (creates new service + ingress URL):**

```bash
./deploy.sh
```

**Update existing service (preserves ingress URL):**

```bash
./deploy.sh --update
```

**Local development mode:**

```bash
./deploy.sh --local
```

**Using a specific connection:**

```bash
./deploy.sh --connection my-connection
```

The deploy script will:

1. ‚úÖ Test Snowflake connection
2. üê≥ Build Docker image for linux/amd64
3. üè∑Ô∏è Tag image for Snowflake registry
4. üîê Authenticate with Snowflake registry
5. ‚¨ÜÔ∏è Push image to repository
6. üì§ Upload service specification
7. üöÄ Create or update the service
8. ‚è≥ Wait for service to be ready
9. üîó Display service endpoint URL

#### Step 5: Create Work Pool in Prefect Cloud

Before configuring the service, create a work pool. Choose based on your deployment method:

**For Snowflake SPCS deployment:**

- Name: `spcs-process`, Type: **Process**

**For Google VM deployment (current active setup):**

- Name: `google-vm`, Type: **Process** (direct execution)
- Name: `google-vm-docker`, Type: **Docker** (containerized flows)

Via CLI:

```bash
uv run prefect cloud login -k pnu_your_key

# For SPCS
uv run prefect work-pool create spcs-process --type process

# For Google VM
uv run prefect work-pool create google-vm --type process
uv run prefect work-pool create google-vm-docker --type docker
```

üìñ **Detailed guide:** See [CREATE_WORK_POOL.md](docs/CREATE_WORK_POOL.md)

#### Step 6: Configure Prefect Environment Variables

After deployment, set your Prefect Cloud credentials:

```sql
-- Set Prefect API URL
ALTER SERVICE E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE SET
  PREFECT_API_URL = 'https://api.prefect.cloud/api/accounts/<your-account-id>/workspaces/<your-workspace-id>';

-- Set Prefect API Key
ALTER SERVICE E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE SET
  PREFECT_API_KEY = 'pnu_your_api_key_here';

-- Set Work Pool (optional, defaults to "spcs-process")
ALTER SERVICE E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE SET
  PREFECT_WORK_POOL = 'spcs-process';

-- Restart service to apply changes
ALTER SERVICE E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE SUSPEND;
ALTER SERVICE E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE RESUME;
```

**Get your Prefect credentials:**

- Log into [Prefect Cloud](https://app.prefect.cloud)
- API URL: Settings ‚Üí General ‚Üí Copy API URL
- API Key: Settings ‚Üí API Keys ‚Üí Create API Key

üìñ **Detailed Prefect setup:** See [PREFECT_SETUP.md](docs/PREFECT_SETUP.md)

#### Step 7: Verify Agent Connection

Check the logs to confirm your Prefect agent is connected:

```bash
snow sql -q "SELECT SYSTEM\$GET_SERVICE_LOGS('E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE', '0', 'e2e-flow-container', 100);" -c default
```

You should see:

```text
üöÄ Starting Prefect worker on Snowflake SPCS...
‚úÖ Connected to Prefect Cloud: https://api.prefect.cloud/api/accounts/...
‚úÖ Work Pool: spcs-process
‚úÖ Starting Prefect worker...
   Running: prefect worker start --pool spcs-process

Starting worker connected to https://api.prefect.cloud/api/accounts/...
Worker started! Looking for work from work pool 'spcs-process'...
```

Then check Prefect Cloud ‚Üí **Work Pools** ‚Üí **spcs-process** to see your worker online! üéâ

## üìã Commands Reference

### üêô GitHub Container Registry Commands

```bash
# Build and push to GitHub Container Registry
./build-and-push-github.sh

# Build only (don't push)
./build-and-push-github.sh --local

# Build with custom tag
./build-and-push-github.sh --tag v1.0.0

# Build without cache
./build-and-push-github.sh --no-cache

# Show help
./build-and-push-github.sh --help
```

### üöÄ Snowflake SPCS Deployment Commands

```bash
# First deployment (creates new service + ingress URL)
./deploy.sh

# Update existing service (preserves ingress URL)
./deploy.sh --update

# Local development mode (build only)
./deploy.sh --local

# Use specific Snowflake connection
./deploy.sh --connection my-connection

# Show help
./deploy.sh --help
```

### üê≥ Local Testing Commands

```bash
# Build and run container
./test-local-container.sh --build

# Run with existing image
./test-local-container.sh

# View logs
./test-local-container.sh --logs

# Open shell in container
./test-local-container.sh --shell

# Stop container
./test-local-container.sh --stop

# Run tests
./test-local-container.sh --test
```

### üîç Service Management (using Snowflake CLI)

```bash
# Check service status
snow sql -q "SELECT SYSTEM\$GET_SERVICE_STATUS('E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE');" -c default

# View service logs (last 100 lines)
snow sql -q "SELECT SYSTEM\$GET_SERVICE_LOGS('E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE', '0', 'e2e-flow-container', 100);" -c default

# Show service endpoints
snow sql -q "SHOW ENDPOINTS IN SERVICE E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE;" -c default

# Show all services
snow sql -q "SHOW SERVICES IN SCHEMA E2E_FLOW_DB.APP_SCHEMA;" -c default

# Show images in repository
snow sql -q "SHOW IMAGES IN IMAGE REPOSITORY E2E_FLOW_DB.IMAGE_SCHEMA.IMAGE_REPO;" -c default

# Suspend service (stops billing)
snow sql -q "ALTER SERVICE E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE SUSPEND;" -c default

# Resume service
snow sql -q "ALTER SERVICE E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE RESUME;" -c default

# Show compute pools
snow sql -q "SHOW COMPUTE POOLS;" -c default
```

### üîß Container Features

- **Base Image:** Python 3.13 Slim
- **Python Version:** 3.13
- **Package Manager:** UV (ultra-fast Python package installer)
- **Architecture:** linux/amd64 (required for Snowflake SPCS)
- **Port:** 8080 (configurable)
- **Prefect:** Worker connects to Prefect Cloud
- **Network:** Outbound HTTPS (port 443) enabled for Prefect Cloud

### üì¶ UV Benefits

- **Speed:** 10-100x faster than pip
- **Reliability:** Better dependency resolution
- **Compatibility:** Drop-in replacement for pip
- **Efficiency:** Parallel downloads and installations

### üèóÔ∏è Multi-Stage Build Benefits

The Dockerfile uses a multi-stage build approach with two stages:

**Builder Stage:**

- Installs build tools (gcc, g++, git)
- Creates virtual environment with all dependencies
- Compiles native extensions if needed

**Runtime Stage:**

- Only copies the compiled virtual environment
- Minimal runtime dependencies (libgomp1, ca-certificates)
- No build tools = smaller image size

**Advantages:**

- **Smaller images:** 40-60% reduction in final image size
- **Faster deployments:** Less data to push/pull
- **Better security:** Reduced attack surface (no compiler tools)
- **Lower costs:** Reduced storage and transfer costs

### üõ†Ô∏è Customization

#### Adding Python Dependencies

Edit `pyproject.toml`:

```toml
[project]
dependencies = [
    "prefect>=2.14.0",
    "prefect-snowflake>=0.27.0",
    # Add your additional dependencies here
    "pandas>=2.0.0",
    "requests>=2.31.0",
]
```

#### Configuring Prefect Worker

The worker is configured via environment variables. Set them in Snowflake:

```sql
-- Required
ALTER SERVICE E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE SET
  PREFECT_API_URL = 'your-prefect-api-url';

ALTER SERVICE E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE SET
  PREFECT_API_KEY = 'your-prefect-api-key';

-- Optional: Specify work pool
ALTER SERVICE E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE SET
  PREFECT_WORKER_POOL_NAME = 'my-spcs-pool';
```

#### Modifying Resources

Edit `spcs-service-spec.yaml`:

```yaml
resources:
  requests:
    memory: 2Gi  # Adjust as needed
    cpu: 1.0     # Adjust as needed
```

#### Changing Python Version

Edit the base image in both stages of the `Dockerfile`:

```dockerfile
# Stage 1: Build stage
FROM python:3.13-slim AS builder  # Change to desired version

# Stage 2: Runtime stage
FROM python:3.13-slim  # Change to match builder version
```

### üìö Additional Resources

- [Snowflake SPCS Documentation](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview)
- [UV Documentation](https://github.com/astral-sh/uv)
- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/)

### üêõ Troubleshooting

**Snowflake CLI not found:**

```bash
# Install Snowflake CLI
brew install snowflake-cli  # macOS
# or
pip install snowflake-cli-labs
```

**Connection issues:**

```bash
# List available connections
snow connection list

# Test connection
snow connection test -c default

# Add new connection
snow connection add
```

**Build fails:**

- Ensure Docker is running
- Check Docker has enough resources (memory/CPU)
- Verify platform is set to linux/amd64

**Deployment fails:**

- Check Snowflake connection: `snow connection test -c default`
- Verify compute pool is active: `SHOW COMPUTE POOLS;`
- Check image repository exists: `SHOW IMAGE REPOSITORIES;`
- Ensure proper privileges (need SPCS permissions)

**Service fails to start:**

```bash
# Check compute pool status
snow sql -q "DESCRIBE COMPUTE POOL E2E_FLOW_COMPUTE_POOL;" -c default

# Check service status
snow sql -q "SELECT SYSTEM\$GET_SERVICE_STATUS('E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE');" -c default

# View service logs
snow sql -q "SELECT SYSTEM\$GET_SERVICE_LOGS('E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE', '0', 'e2e-flow-container', 100);" -c default

# Verify image exists
snow sql -q "SHOW IMAGES IN IMAGE REPOSITORY E2E_FLOW_DB.IMAGE_SCHEMA.IMAGE_REPO;" -c default
```

**Service endpoint not working:**

- Wait 1-2 minutes for service to be fully ready
- Check service status shows "READY"
- Verify endpoint is public in spcs-service-spec.yaml
- Check container logs for errors

**Container crashes:**

- Review logs: `./deploy.sh` output or `snow sql` logs command
- Test locally first: `./test-local-container.sh --build`
- Check resource limits in spcs-service-spec.yaml
- Verify all dependencies are in pyproject.toml

### üí° Best Practices

1. **Always use `--update` for subsequent deployments** to preserve your ingress URL
2. **Test locally first** with `./test-local-container.sh --build` before deploying
3. **Monitor your service** regularly using status and logs commands
4. **Suspend services** when not in use to save costs
5. **Use appropriate resource limits** in spcs-service-spec.yaml
6. **Version your images** by changing IMAGE_TAG in deploy.sh

### üîÑ Development Workflow

1. **Make code changes** to your application
2. **Test locally:**

   ```bash
   ./test-local-container.sh --build
   ```

3. **Deploy updates:**

   ```bash
   ./deploy.sh --update
   ```

4. **Monitor deployment:**

   ```bash
   snow sql -q "SELECT SYSTEM\$GET_SERVICE_STATUS('E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE');" -c default
   ```

5. **Check logs if needed:**

   ```bash
   snow sql -q "SELECT SYSTEM\$GET_SERVICE_LOGS('E2E_FLOW_DB.APP_SCHEMA.E2E_FLOW_SERVICE', '0', 'e2e-flow-container', 100);" -c default
   ```

### üìö Documentation

#### Quick References

- **[QUICKSTART.md](docs/QUICKSTART.md)** - 10-minute quick start guide
- **[DEPLOYMENT_GUIDE.md](docs/DEPLOYMENT_GUIDE.md)** - Comprehensive deployment guide
- **[PREFECT_SETUP.md](docs/PREFECT_SETUP.md)** - Detailed Prefect worker configuration

#### Prefect Configuration

- **[CREATE_WORK_POOL.md](docs/CREATE_WORK_POOL.md)** - Creating work pools in Prefect Cloud
- **[PREFECT_AUTH.md](docs/PREFECT_AUTH.md)** - Authentication methods explained
- **[GET_PREFECT_URL.md](docs/GET_PREFECT_URL.md)** - Getting your Prefect API URL

#### Technical Details

- **[DOCKERFILE_CHANGES.md](docs/DOCKERFILE_CHANGES.md)** - Dockerfile updates and optimization
- **[PREFECT_CHANGES.md](docs/PREFECT_CHANGES.md)** - Prefect integration changes
- **[CHANGES.md](docs/CHANGES.md)** - Complete changelog

#### External Resources

- [Prefect Documentation](https://docs.prefect.io)
- [Prefect Cloud](https://app.prefect.cloud)
- [Snowflake SPCS Documentation](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview)
- [Snowflake CLI Documentation](https://docs.snowflake.com/en/developer-guide/snowflake-cli/index)
- [UV Documentation](https://github.com/astral-sh/uv)
- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/)
- [Reference Implementation](https://github.com/sfc-gh-jkang/cortex-cost-app-spcs)

### üìù Next Steps

- [x] Configure Prefect integration ‚úÖ
- [x] Enable outbound HTTPS for Prefect Cloud ‚úÖ
- [x] Create Prefect flows and deployments ‚úÖ (4 deployments in prefect.yaml)
- [x] EVE Online market data ETL pipeline ‚úÖ
- [x] Dual database support (Crunchy + Snowflake) ‚úÖ
- [x] CSV loader with upsert support ‚úÖ
- [x] Docker-based deployments ‚úÖ
- [ ] Add health check endpoint
- [ ] Implement monitoring and alerting
- [ ] Set up CI/CD pipeline
- [ ] Add OpenFlow CDC replication to Snowflake tables

---

**üéâ You now have a Prefect worker running!** The worker connects to Prefect Cloud and executes your flow runs.

**Current active setup:** Google VM with Docker-based ETL flows loading EVE Online market data to Crunchy Bridge and Snowflake PostgreSQL databases.
